{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turner Sandstone Production Prediction\n",
    "Last week we looked at data wrangling to get our data organized and ready for machine learning. We now have a full `DataFrame` with values for each feature for each well. First we are going to import pandas to read in our munged data, and a few functions from scikit-learn to do the heavy lifting for the machine learning portion of the notebook. We are going to use `RandomForestRegressor` to find out which features are most important for predicting the first 18 months of oil production. We will use `train_test_split` to split our data into training and test sets to validate our model, and `RandomizedSearchCV` will be used to find the best hyperparameters for our random forest regressor. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and read in our munged data from the `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "munged_data = pd.read_csv(\"organized_turner_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by setting the value we want to predict (production) to a `numpy array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = munged_data[\"First 18 months oil (bbl)\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's remove the production values from our `dataframe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = munged_data.drop([\"First 18 months oil (bbl)\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split the data into training and test datasets. This way the model won't see all the data during the training phase and we can validate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.01, random_state=19\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up a random grid of hyperparameters for `RandomizedSearchCV` to wander through to find the best predictive model. A full description is found in the scikit-learn docs at https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of trees in the forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=100, stop=900, num=100)]\n",
    "# the number of features to use at each split\n",
    "max_features = [\"auto\", \"sqrt\"]\n",
    "# max number of levels in each tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 220, num=11)]\n",
    "max_depth.append(None)\n",
    "# minimum samples needed to split a tree\n",
    "min_samples_split = [2, 5, 10, 15, 20]\n",
    "# minimum samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4, 8, 16]\n",
    "# method for selecting samples\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# create the grid\n",
    "random_grid = {\n",
    "    \"n_estimators\": n_estimators,\n",
    "    \"max_features\": max_features,\n",
    "    \"max_depth\": max_depth,\n",
    "    \"min_samples_split\": min_samples_split,\n",
    "    \"min_samples_leaf\": min_samples_leaf,\n",
    "    \"bootstrap\": bootstrap,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will fit our `RandomForestRegressor` to the training data, and use 3 fold cross validation for 100 iterations. This finds the best hyperparameters for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   32.1s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [100, 108, 116, 124, 132, 140, 148, 156, 164, 172, 180, 188, 196, 205, 213, 221, 229, 237, 245, 253, 261, 269, 277, 285, 293, 302, 310, 318, 326, 334, 342, 350, 358, 366, 374, 382, 390, 398, 407, 415, 423, 431, 439, 447, 455, 463, 471, 479, 487, 495, 504, 512, 52...amples_split': [2, 5, 10, 15, 20], 'min_samples_leaf': [1, 2, 4, 8, 16], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=19, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=random_grid,\n",
    "    n_iter=100,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=19,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the best fit parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 196,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 178,\n",
       " 'bootstrap': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much better are the best fit parameters compared to the standard 'out of the box' version of the `RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 13141.2128 barrels\n",
      "Accuracy = 51.45%.\n",
      "Model Performance\n",
      "Average Error: 5187.3445 barrels\n",
      "Accuracy = 80.99%.\n"
     ]
    }
   ],
   "source": [
    "def compare(model, test_features, test_actual):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_actual)\n",
    "    mape = 100 * np.mean(errors / test_actual)\n",
    "    accuracy = 100 - mape\n",
    "    print(\"Model Performance\")\n",
    "    print(\"Average Error: {:0.4f} barrels\".format(np.mean(errors)))\n",
    "    print(\"Accuracy = {:0.2f}%.\".format(accuracy))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "base_model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "base_model.fit(X_train, y_train)\n",
    "base_accuracy = compare(base_model, X_train, y_train)\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = compare(best_random, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is 74.47% accurate on the test data\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"The model is {:0.2f}% accurate on the test data\".format(\n",
    "        100 * best_random.score(X_test, y_test)\n",
    "    )\n",
    ")  # let's check the model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model does ok on the test data, let's use the `feature_importances_` of the `best_random` model to see which features are most important in predicting the first 18 months of oil production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Total vertical depth at bottom hole (ft)', 0.11960542352545994),\n",
       " ('Surface hole longitude (NAD83)', 0.0809194686492954),\n",
       " ('Bottom hole longitude (NAD83)', 0.07627055885098187),\n",
       " ('Average gas-oil ratio (ft3/bbl)', 0.07529668554350513),\n",
       " ('Total proppant (lb)', 0.07128679299929348),\n",
       " ('Surface-to-bottom hole length (ft)', 0.06390807852259404),\n",
       " ('APINO', 0.05245075239719613),\n",
       " ('Unnamed: 0', 0.051164358081107676),\n",
       " ('Number of frac stages', 0.05073672963814896),\n",
       " ('Producing interval length (ft)', 0.050415724278201346),\n",
       " ('IP oil API gravity (°)', 0.048883895694935495),\n",
       " ('API number', 0.047979678905179915),\n",
       " ('Bottom hole latitude (NAD83)', 0.04744873764261307),\n",
       " ('Surface hole latitude (NAD83)', 0.04461134548586631),\n",
       " ('Lateral azimuth (°)', 0.041852140214180794),\n",
       " ('Total slurry (bbl)', 0.03755086066764833),\n",
       " ('Calculated top of reservoir temperature (°F)', 0.018357989410994084),\n",
       " ('Company', 0.016595436599855982),\n",
       " ('WSGS reservoir', 0.003914409965170883),\n",
       " ('Well type', 0.0007509329277711771)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_labels = X.columns.values  # get the feature labels\n",
    "feature = list(\n",
    "    zip(feat_labels, best_random.feature_importances_)\n",
    ")  # make a list of the feature labels and the importance values\n",
    "sorted(\n",
    "    feature, key=lambda tup: tup[1], reverse=True\n",
    ")  # sort from most to least important feature in predicting production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the `Total Vertical Depth` of a well is the most important feature in predicting the first 18 months of a wells oil production followed by the `Surface hole longitude`, `Bottom hole longitude` and the `Total Proppant`. If we assume that TVD and location are considered geologic factors then geology has more influence in predicting production than the completion design of these Turner Sandstone wells in the Powder River Basin. This agrees with the conclusions of [RI-77](http://sales.wsgs.wyo.gov/influences-on-oil-and-natural-gas-production-from-the-wall-creek-and-turner-sandstone-reservoirs-powder-river-basin-wyoming-2019/?ctk=2a2030c2-bb60-4a4d-aa30-14502a0aee7c) that the complex geology of the reservoir should be considered when designing these wells. \n",
    "\n",
    "It's interesting seeing that the `Company` feature and the `WSGS reservoir` have some of the least importance in predicting the first 18 months of production. There you have it, we took a messy spreadsheet, munged the data in it, and then used the munged data to come to the same conclusion as the report in about 40 lines of Python. \n",
    "### Yay for reproducibility!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is licensed as CC-BY, use and share to your hearts content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
